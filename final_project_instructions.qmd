---
title: "30538 Final Project: Reproducible Research"
author: "Peter Ganong and Maggie Shi" 
date: "today"
format: pdf
execute:
  eval: false
  echo: false
---

# Project Description and Instructions
The goal of this project is to showcase your knowledge of Python by applying it to a research project about a policy topic you are interested in. You will be graded on coding, writeup, and an in-class presentation.

You may work on this project alone, or in groups of up to three students. All groups must be formed declared in the Canvas proposal before any work is done - it is not possible to join one after. 

It is required that you use GitHub, and we may use your past commits to understand your thought process for partial credit. If you working in a group, note that as we are grading we will be looking for multiple commits per individual throughout the project. The division of labor should be approximately evenly across both individuals. While we will lean toward giving the same grade for all group members, it is possible that individuals may receive different grades based on the commit history.

If you choose to form a group, we recommend that you do so with other students in your section. You are allowed to have a group member from another section, but all group members must be available to attend all the group members' lecture sessions. If you are not present when your project is presented, you will not receive credit for the presentation.


# Grading 
## Coding (70%)
The code for the project should have the following components:

1. Data wrangling (25%)
    * You must use a minimum of *two* datasets. 
    * All processing of the data should be handled by your `.qmd` code, including all merging and reshaping. 

```{python}
# General
import pandas as pd
import numpy as np

# Visualization 
import altair as alt

# Geospatial data handling
import geopandas as gpd

# shiny framework
from shiny import App, ui, render

#for spatial data
from us import states
```

```{python}
#Importing the data - AmeriCorps 2021 CEV 
#Source: https://data.americorps.gov/dataset/2021-CEV-Data-Current-Population-Survey-Civic-Enga/rgh8-g2uc/about_data
#Will need to include google drive/dropbox link since > 300MB

#cev_2021_raw = pd.read_csv("/Users/charleshuang/Documents/GitHub/student30538/problem_sets/final_project/data/2021_CEV__Current_Population_Survey_Civic_Engagement_and_Volunteering_Supplement_20241031.csv", encoding = 'utf-8')

cev_2021_raw = pd.read_csv(
    "C:/Users/andre/Documents/GitHub/ppha30538_final_project/data/2021_CEV_Data__Current_Population_Survey_Civic_Engagement_and_Volunteering_Supplement_20241127.csv", encoding = 'utf-8'
    )

#vcl_supplement_raw = pd.read_csv("/Users/charleshuang/Documents/GitHub/student30538/problem_sets/final_project/data/sep21pub.csv")

vcl_supplement_raw = pd.read_csv("C:/Users/andre/Documents/GitHub/ppha30538_final_project/data/sep21pub.csv")


```
Most relevant variables to focus on:

Frequency and Type of Volunteering:
PES16: Did the respondent spend any time volunteering for any organization in the past 12 months?
PES16D: Frequency of volunteering (e.g., basically every day, a few times a week).
PTS16E: Approximate hours spent volunteering.
Political Engagement:
PES2: How often the respondent discussed political, societal, or local issues with friends or family.
PES5: How often these discussions occurred with neighbors.
PES13: Contact or visits to a public official to express opinions.
PES14: Boycotting or buying products based on political values or business practices.
Civic Participation and Group Membership:
PES15: Belonging to groups, organizations, or associations in the past 12 months.
Neighbor and Community Interaction:
PES7: Participation in activities to improve their neighborhood or community.
Voting Behavior:
PES11: Whether the respondent voted in the last local elections.
Social Media and News Consumption:
PES9: Posting views about political, societal, or local issues on the internet or social media.
PES10: Frequency of consuming news related to political or societal issues.

Basic Demographics
Age: PRTAGE (Person's age)
Gender: PESEX (Sex of the respondent)
Race/Ethnicity: PTDTRACE (Detailed race and Hispanic origin)
Marital Status: PEMARITL (Marital status of the respondent)
Household Composition: HRNUMHOU (Number of persons in the household)

Potential Confounding Variables
Income: HEFAMINC (Household family income level)
Education: PEEDUCA (Highest level of school completed)
Urban/Rural Status: GTMETSTA (Metropolitan or non-metropolitan status)
Community Involvement: PES7 (Participation in neighborhood or community activities)
Social Media Use: PES9 (Posting views about political, societal, or local issues on the internet or social media)

This code will filter out the 400+ variables in the dataset to only the relevant ones.

First we need to merge the two datasets:

```{python}
vcl_supplement_raw.columns = vcl_supplement_raw.columns.str.lower()

# I found out the data type is int64 for one df and object for the other

cev_2021_raw = cev_2021_raw.astype(str)
vcl_supplement_raw = vcl_supplement_raw.astype(str)

# This finds all the variables in common between the two for a merge
common_keys = list(set(vcl_supplement_raw.columns).intersection(
    set(cev_2021_raw.columns)))


cev_all_2021 = pd.merge(cev_2021_raw, vcl_supplement_raw,
                        on=common_keys, how="outer")

cev_all_2021.head(5)

# Note: Each row is a person- multiple people in the same household can have same household ID, so should not filter by unique

# We can use a config.py file to keep this readable. Full details are in the config.py file
``` 

```{python}
from config import selected_variables, rename_mapping


# selected_variables = ['hrhhid', 'hrhhid2', etc.]
# This chooses the variables we want from the merged raw data

# rename_mapping = "hrhhid": "Household_ID", "hrhhid2": "Household_ID_2", etc.
# Renaming the variables for clarity


cev_all_2021_filter = cev_all_2021[selected_variables].copy()
#copy avoids potentially modifying original dataframe

cev_all_2021_filter.rename(columns=rename_mapping, inplace=True)

#Debugging: removing duplicate column(s)
cev_all_2021_filter = cev_all_2021_filter.loc[:, ~cev_all_2021_filter.columns.duplicated()]
```


The data is a mix of numeric code and qualitative input, so we need to keep the qualitative input while mapping only numeric codes. The qualitative input is outside of the data dictionary, so it won't get picked up by any mapping functions and will be transmuted into NaN data. We will make a function that identifies all the values in the data that aren't picked up by our data dictionaries in config.py.

For example, here's a sample from config.py:

Renaming pes16 - Volunteered Past Year
In the past 12 months, did [you/[NAME]] spend any time volunteering for any organization or association?

pes16_dict = {
    '1': 'Yes',
    '2': 'No',
    '-1': 'Not in Universe',
    '-3': 'Refusal',
    '-2': 'Do Not Know',
    '-9': 'No Answer',
    '.u': 'No Answer',
    '.r': 'Refusal',
    '.n': 'Not in Universe',
    '.d': 'Do Not Know'
}

If there are some entries in this column that are already coded "Yes" or "No", our existing mapping won't account for them and will turn them into NANs. That's not desirable. We want to catch those and account for them.

```{python}
#For testing purposes, delete later
cev_pre_clean_test = cev_all_2021_filter.copy()

```

```{python}

# Attribution: Asked ChatGPT "why isn't config.py updating when I add dictionaries to it; ChatGPT suggested using importlib reload"
# Import and reload config
from pathlib import Path
import os
from us import states
from config import add_engagement_score
from config import *
import importlib
import config
importlib.reload(config)


# Replace FIPS codes with states

fips_to_state = {int(state.fips): state.abbr for state in states.STATES}

cev_all_2021_filter['US State'] = pd.to_numeric(
    cev_all_2021_filter['US State'], errors='coerce')

cev_all_2021_filter['US State'] = cev_all_2021_filter['US State'].map(
    fips_to_state)

# Replace "Volunteered Past Year" data (pes16)
cev_all_2021_filter['Volunteered_Past_Year'] = cev_all_2021_filter['Volunteered_Past_Year'].map(
    pes16_dict)

# Replace "Volunteering Frequency" (pes16d)
cev_all_2021_filter['Volunteering_Frequency'] = cev_all_2021_filter['Volunteering_Frequency'].map(
    pes16d_dict)

# Replace "Hours Spent Volunteering" (pts16e)
cev_all_2021_filter['Hours_Spent_Volunteering'] = cev_all_2021_filter['Hours_Spent_Volunteering'].map(
    pts16e_dict)

# Replace "Discussed Issues with Friends/Family" data (pes2)
cev_all_2021_filter['Discussed_Issues_With_Friends_Family'] = cev_all_2021_filter['Discussed_Issues_With_Friends_Family'].map(
    pes2_dict)

# Replace "Discussed Issues with Neighbors" data (pes5)
cev_all_2021_filter['Discussed_Issues_With_Neighbors'] = cev_all_2021_filter['Discussed_Issues_With_Neighbors'].map(
    pes5_dict)

# Replace "Contacted Public Official" data (pes13)
cev_all_2021_filter['Contacted_Public_Official'] = cev_all_2021_filter['Contacted_Public_Official'].map(
    pes13_dict)

# Replace "Boycott Based on Values" data (pes14)
cev_all_2021_filter['Boycott_Based_On_Values'] = cev_all_2021_filter['Boycott_Based_On_Values'].map(
    pes14_dict)

# Replace "Belonged to Groups" data (pes15)
cev_all_2021_filter['Belonged_To_Groups'] = cev_all_2021_filter['Belonged_To_Groups'].map(
    pes15_dict)


# Replace "Community Improvement Activities" data (pes7)
cev_all_2021_filter['Community_Improvement_Activities'] = cev_all_2021_filter['Community_Improvement_Activities'].map(
    pes7_dict)

# Replace "Voted in Local Election" data (pes11)
cev_all_2021_filter['Voted_In_Local_Election'] = cev_all_2021_filter['Voted_In_Local_Election'].map(
    pes11_dict)

# Replace "Posted Views on Social Media" data (pes9)
cev_all_2021_filter['Posted_Views_On_Social_Media'] = cev_all_2021_filter['Posted_Views_On_Social_Media'].map(
    pes9_dict)

# Replace "Frequency of News Consumption" data (pes10)
cev_all_2021_filter['Frequency_Of_News_Consumption'] = cev_all_2021_filter['Frequency_Of_News_Consumption'].map(
    pes10_dict)

# Replace "Age" data (prtage)
cev_all_2021_filter['Age'] = cev_all_2021_filter['Age'].map(prtage_dict)

# Replace "Gender" data (pesex)
cev_all_2021_filter['Gender'] = cev_all_2021_filter['Gender'].map(pesex_dict)

# Replace "Race/Ethnicity" data (ptdtrace)
cev_all_2021_filter['Race_Ethnicity'] = cev_all_2021_filter['Race_Ethnicity'].map(
    ptdtrace_dict)

# Replace "Marital Status" data (pemaritl)
cev_all_2021_filter['Marital_Status'] = cev_all_2021_filter['Marital_Status'].map(
    pemaritl_dict)

# Replace "Household Size" data (hrnumhou)
cev_all_2021_filter['Household_Size'] = cev_all_2021_filter['Household_Size'].map(
    hrnumhou_dict)

# Replace "Family Income Level" data (hefaminc)
cev_all_2021_filter['Family_Income_Level'] = cev_all_2021_filter['Family_Income_Level'].map(
    hefaminc_dict)

# Replace "Education Level" data (peeduca_dict)
cev_all_2021_filter['Education_Level'] = cev_all_2021_filter['Education_Level'].map(
    peeduca_dict)

# Replace "Urban Rural Status" data (gtmetsta_dict)
cev_all_2021_filter['Urban_Rural_Status'] = cev_all_2021_filter['Urban_Rural_Status'].map(
    gtmetsta_dict)

# Debugging code - Before engagement score calculation
print("Columns before adding engagement score:")
print(cev_all_2021_filter.columns.tolist())
print("\nShape before:", cev_all_2021_filter.shape)


# Mutate the engagement score variable we came up with in config.py
cev_all_2021_filter = add_engagement_score(cev_all_2021_filter)


# More debugging code
print("\nColumns after adding engagement score:")
print(cev_all_2021_filter.columns.tolist())
print("\nShape after:", cev_all_2021_filter.shape)

# Before saving
print("\nVerifying engagement score columns exist:")
print("'political_engagement_score' exists:",
      'political_engagement_score' in cev_all_2021_filter.columns)
print("'engagement_level' exists:",
      'engagement_level' in cev_all_2021_filter.columns)


# Lastly, export the cleaned data as a csv for plotting purposes


shiny_data_path = Path("shiny-app/basic-app/data")

# Create directory if it doesn't exist
shiny_data_path.mkdir(parents=True, exist_ok=True)

# Save with explicit column list to ensure all columns are included (debugging)
columns_to_save = cev_all_2021_filter.columns.tolist()
cev_all_2021_filter.to_csv(
    shiny_data_path / "cev_2021_cleaned.csv",
    columns=columns_to_save,
    index=False
)

# Verify saved file - more debugging
verification_df = pd.read_csv(shiny_data_path / "cev_2021_cleaned.csv")
print("\nColumns in saved CSV:")
print(verification_df.columns.tolist())

print(f"Dataset saved to: {shiny_data_path / 'cev_2021_cleaned.csv'}")

```

```{python}
#For summary/debugging only
print((cev_all_2021_filter["Volunteered_Past_Year"] == "Yes").sum())
print(21482/255744)

print(cev_all_2021_filter['political_engagement_score'].mean())
```

```{python}


```


Read in data from American National Election Studies
```{python}
polarization = pd.read_csv("C:/Users/andre/Documents/GitHub/ppha30538_final_project/data/anes_timeseries_2020_csv_20220210.csv")
```

Important elements of the ANES Codebook

1. Note on geography:

"Some variables have been removed from the full release dataset in order to protect
 respondent confidentiality....
 Examples of restricted variables include, but are not limited to: detailed geography,
 detailed religious denomination..."

2. Understanding the variables:
" The Codebook includes the following information, where applicable, for each
 variable: the variable name (all variable names start with a “V”, and summary
 variables end with the suffix “x”), variable label (with “PRE:” indicating that it is from
 the pre-election study), question wording/variable meaning, response/code values
 and meanings, universe (where only a subset of respondents appear in a variable,
 which respondents those should be; note that universes are not included for most
 summary variables), associated survey question(s), information about whether
 randomization was used, interviewer instructions, and any other notes about the
 variable"


Subset for the useful columns

List 1: This list contains only pre-election data, it is designed to capture variables
covering some geographic information (V201011, V201013a, V201013b, V201014a, V201014b)

List 2: This list contains only pre-election data, it is designed to capture variables
covering information about assessments of political positioning
(i.e. left, right, center)
```{python}
#list 1
subset_list_1 = [column for column in polarization if "V2010" in column]

#list 2
subset_list_2 = [column for column in polarization if "V2012" in column]

#put the lists together
#make a loop 

#empty receiver
subset_list = []

#loop 1
for value in subset_list_1:
        subset_list.append(value)

#loop 2
for value in subset_list_2:
        subset_list.append(value)                    

#subset 
sub_polarization = polarization.filter(items= subset_list)

#add a column that just equals 1 to use for tracking the number
#of entries
sub_polarization["Observations"] = 1
```

Some more data cleaning

Analyzing Question V201200, which is a question asking:

"Where would you place yourself on this scale, or haven’t you
 thought much about this?
 Value Labels-9. Refused -8. Don’t know 
1. Extremely liberal 
2. Liberal 
3. Slightly liberal 
4. Moderate; middle of the road 
5. Slightly conservative 
6. Conservative 
7. Extremely conservative 
99. Haven’t thought much about this"

```{python}
#Make variables more clear

crosswalk_polar = pd.DataFrame({
    "Self_Rating_(V201200)":[-9, -8, 1, 2, 3, 4, 5, 6, 7, 99],
    "Ideology_(V201200)": [
        "Refused", "Don't Know", "Extremely Liberal",
        "Liberal", "Slightly Liberal", 
        "Moderate; middle of the road",
        "Slightly conservative",
        "Conservative",
        "Extremely conservative",
        "Haven’t thought much about this"
    ]
    })

#merge using crosswalk
sub_polarization = sub_polarization.merge(crosswalk_polar, left_on = "V201200", right_on = "Self_Rating_(V201200)")

```

Make dataframe where all data is aggregated by state, and then
we can show correlation between measure of polarity and 
the share of respondents in a state who did volunteer work.

Clean data to produce some geographic information. Note that we have two US State variables, with only
"US State 2" being used in analysis, purely because it has more in-universe entries. This appears to 
be partially due to respondent reactions to different questions, and partially due to information 
restrictions on the dataset.

V201013a
```{python}
#Replace FIPS codes with states
from us import states

fips_to_state = {int(state.fips): state.abbr for state in states.STATES}

sub_polarization['US State'] = pd.to_numeric(sub_polarization['V201013a'], errors='coerce')

sub_polarization['US State'] = sub_polarization['US State'].map(fips_to_state)

sub_polarization["US State 2"] = pd.to_numeric(sub_polarization['V201014b'], errors='coerce')

sub_polarization['US State 2'] = sub_polarization['US State 2'].map(fips_to_state)


```

Then, make data using V201018, which asks:  What political party are you registered with, if any?

-9. Refused -8. Don’t know -1. Inapplicable 
1 Democratic party 
2 Republican party
 4  None or ‘independent’ 
5 Other {SPECIFY}

```{python}
crosswalk_party = pd.DataFrame({
    "Party_Numbers_(V201018)":[-9, -8, -1, 1, 2, 4, 5],
    "Party_Affiliation_(V201018)":[
        "Refused", "Don't know", "Inapplicable", "Democratic party",
        "Republican party", "None or independent", "Other"
        ]})

sub_polarization = sub_polarization.merge(crosswalk_party, left_on = "V201018", right_on = "Party_Numbers_(V201018)")

```

Going to switch to a different marker for political party due to some challenges with V201018
Now using: V201228, which asks:

"Generally speaking, do you usually think of yourself as [a
Democrat, a Republican / a Republican, a Democrat], an
independent, or what?"

-9. Refused
-8. Don’t know
-4. Technical error
0. No preference {VOL - video/phone only}
1. Democrat
2. Republican
3. Independent
5. Other party {SPECIFY}

```{python}
crosswalk_party_2 = pd.DataFrame({
    "Party_Numbers_(V201228)":[-9, -8, -4, 0, 1, 2, 3, 5],
    "Party_Affiliation_(V201228)":[
        "Refused", "Don't know", "Technical error", "No Preference", "Democrat",
        "Republican", "Independent", "Other party"
        ]})

sub_polarization = sub_polarization.merge(crosswalk_party_2, left_on = "V201228", right_on = "Party_Numbers_(V201228)")


```


Following that, make function that subsets by state, and then groups
by political party, and then returns the share of Democrats
that are some kind of Conservative, and the share of Republicans that are some kind of Liberal
```{python}


def find_share_state(df, state):
        #subset by state
        sub = df[df["US State 2"] == state]
        Party_People = ["Democrat", "Republican"]
        Conservatives = ["Slightly conservative",
        "Conservative",
        "Extremely conservative"]
        Liberals = ["Extremely Liberal",
        "Liberal", "Slightly Liberal"]
        #count of partisans
        Party_Count = []
        #count of entries that are Dem but conservative or Repub but liberal
        Outlier_box = []
        #get Party_Count
        for index, entry in sub.iterrows():
                if entry["Party_Affiliation_(V201228)"] in Party_People:
                        Party_Count.append(1)
                else:
                        Party_Count.append(0)
        
        for index, entry in sub.iterrows():
                if (entry["Ideology_(V201200)"] in Conservatives) & (entry["Party_Affiliation_(V201228)"] == "Democratic"):
                        Outlier_box.append(1)
                elif (entry["Ideology_(V201200)"] in Liberals) & (entry["Party_Affiliation_(V201228)"] == "Republican"):
                        Outlier_box.append(1)
                else:
                        Outlier_box.append(0)
        
        sub["Party_Count"] = Party_Count
        sub["Outliers"] = Outlier_box
        return sub
        




```

Make bigger function to produce results by each state by subsetting for a given state
```{python}

def seek_polar(df, state):
        mod_df = find_share_state(df, state)
        mod_df = mod_df.groupby(
            "Party_Affiliation_(V201228)"
            )[["Outliers", "Party_Count"]].sum().reset_index()

        mod_df["Percent_Outliers"] =  mod_df["Outliers"] / sum(mod_df["Party_Count"])
        return mod_df

#Example with IL
seek_polar(sub_polarization, "IL")

```

Make a function returning the number of outliers in a given state's party population as a share of that state's party population and graph

Produce modified "nation" version of find_share()
```{python}
def find_share_nation(df):
        Party_People = ["Democrat", "Republican"]
        Conservatives = ["Slightly conservative",
        "Conservative",
        "Extremely conservative"]
        Liberals = ["Extremely Liberal",
        "Liberal", "Slightly Liberal"]
        #count of partisans
        Party_Count = []
        #count of entries that are Dem but conservative or Repub but liberal
        Outlier_box = []
        #get Party_Count
        for index, entry in df.iterrows():
                if entry["Party_Affiliation_(V201228)"] in Party_People:
                        Party_Count.append(1)
                else:
                        Party_Count.append(0)
        #get outlier count
        for index, entry in df.iterrows():
                if (entry["Ideology_(V201200)"] in Conservatives) & (entry["Party_Affiliation_(V201228)"] == "Democratic"):
                        Outlier_box.append(1)
                elif (entry["Ideology_(V201200)"] in Liberals) & (entry["Party_Affiliation_(V201228)"] == "Republican"):
                        Outlier_box.append(1)
                else:
                        Outlier_box.append(0)
        
        df["Party_Count"] = Party_Count
        df["Outliers"] = Outlier_box
        return df

```

```{python}
def polar_table(df):
        mod_df = find_share_nation(df)
        #make a grouped df that takes sum of each states' outliers and 
        #people who identify with a party
        mod_df = mod_df.groupby("US State 2")[
            ["Outliers", "Party_Count"]
            ].sum().reset_index()
        mod_df["Percent_Outliers"] =  mod_df["Outliers"] / sum(mod_df["Party_Count"])
        return mod_df


```

Save to a dataframe
```{python}
polar_by_party = polar_table(sub_polarization)

#graphing percentages
percent_graph_outliers = alt.Chart(polar_by_party).mark_bar().encode(
    alt.X("US State 2", title = "State"),
    alt.Y("Percent_Outliers", title = "Outlier Share"),
    alt.Color("US State 2")
)

#graphing sums
sum_graph_outliers = alt.Chart(polar_by_party).mark_bar().encode(
    alt.X("US State 2", title = "State"),
    alt.Y("Outliers", title = "Outlier Sum"),
    alt.Color("US State 2")
)

```

In a paper on quantifying polarization written by Aaron Bramson et al (
    https://inferenceproject.yale.edu/sites/default/files/688938.pdf), the authors examine a range of polarization
indicators. A relatively simple (and in some ways problematic) measurement is called spread, or dispersion. 
It's essentially the 
gap between the most extreme political positions. We (imperfectly) approximate this using two more variables:
V201206 and V201207. These ask respondents to position political parties on the political spectrum. We can select
the most ideologically distant nodes on the personal ideology scale (extremely liberal and extremely conservative)
and capture how far apart their conceptions of each party are, and then disaggregate by state.

Crosswalk V201206 and V201207
```{python}
crosswalk_V201206 = pd.DataFrame({
    #the dataset's scaled
    "Dem_Num_(V201206)":[-9, -8, 1, 2, 3, 4, 5, 6, 7],
    #the dataset's ideology
    "Dem_Ideology_(V201206)": [
        "Refused", "Don't Know", "Extremely Liberal",
        "Liberal", "Slightly Liberal", 
        "Moderate; middle of the road",
        "Slightly conservative",
        "Conservative",
        "Extremely conservative"
    ],
    #new values for comparison
    "Dem_Positioning(V201206)":[0, 0, -3, -2, -1, 0, 1, 2, 3]
    })

#They're exactly the same
crosswalk_V201207 = pd.DataFrame({
    #the dataset's scale
    "Repub_Num_(V201207)":[-9, -8, 1, 2, 3, 4, 5, 6, 7],
    #the dataset's ideology
    "Repub_Ideology_(V201207)": [
        "Refused", "Don't Know", "Extremely Liberal",
        "Liberal", "Slightly Liberal", 
        "Moderate; middle of the road",
        "Slightly conservative",
        "Conservative",
        "Extremely conservative"
    ],
    #new values for comparison
    "Repub_Positioning(V201207)":[0, 0, -3, -2, -1, 0, 1, 2, 3]
    })

#crosswalking V201206
sub_polarization = sub_polarization.merge(crosswalk_V201206, left_on = "V201206", right_on = "Dem_Num_(V201206)")

#crosswalking V201207
sub_polarization = sub_polarization.merge(crosswalk_V201207, left_on = "V201207", right_on = "Repub_Num_(V201207)")

#drop Dem_Num and Repub_Num for clarity (we'll be using the reassigned numbers)
sub_polarization = sub_polarization.drop(["Dem_Num_(V201206)", "Repub_Num_(V201207)"], axis = 1)
```

Use the Positioning columns to compare the average party position 
selections between the 2 extremes

Step 1: First grouping
```{python}

#filter out all self-identifiers that aren't "Extremely Liberal"
#or Extremely Conservative
ex_polarization = sub_polarization[
    (sub_polarization["Ideology_(V201200)"] == "Extremely conservative") | 
    (sub_polarization["Ideology_(V201200)"] == "Extremely Liberal")
    ]

#filter out entries of Party Ideology Position that are "Don't Know" and "Refused"
ex_polarization = ex_polarization[
    (sub_polarization["Dem_Ideology_(V201206)"] != "Don't Know") &
    (sub_polarization["Dem_Ideology_(V201206)"] != "Refused") &
    (sub_polarization["Repub_Ideology_(V201207)"] != "Don't Know") &
    (sub_polarization["Repub_Ideology_(V201207)"] != "Refused") 
    ]


#make column wehere L = 0 and C = 1 
Extreme_L_C = [
    "Liberal" if row["Ideology_(V201200)"] == "Extremely Liberal" else "Conservative" 
    for index, row in ex_polarization.iterrows()
    ]

#save to dataframe
ex_polarization["Extreme_Position"] = Extreme_L_C

#groupby Extreme_Position and US State 2 and get average Dem and Repub positioning
position_groups = ex_polarization.groupby(
    ["US State 2", "Extreme_Position"])[
        ["Dem_Positioning(V201206)", "Repub_Positioning(V201207)"]
    ].mean().reset_index()
```

Attribution: Asked ChatGPT for help fixing my list comprehension

Use position_groups 
```{python}
#pivot on Extreme_Position
pivot_position = position_groups.pivot(
    index = "US State 2", 
    columns = "Extreme_Position", 
    values = ["Dem_Positioning(V201206)", "Repub_Positioning(V201207)"]
    )

#reset index
pivot_position = pivot_position.reset_index()

#Flatten multi-index
pivot_position.columns = ['_'.join(col).strip() for col in pivot_position.columns.values]

#rename flattened columns
pivot_position = pivot_position.rename(
        columns = {
            "US State 2_":"US State 2", 
            "Dem_Positioning(V201206)_Conservative":"Dem_Position_C",
            "Dem_Positioning(V201206)_Liberal":"Dem_Position_L",
            "Repub_Positioning(V201207)_Conservative":"Repub_Position_C",
            "Repub_Positioning(V201207)_Liberal":"Repub_Position_L"
        }
    )

#capture spread variables
pivot_position["Spread_Dem"] = abs(
    pivot_position["Dem_Position_C"] - 
    pivot_position["Dem_Position_L"]
    )

pivot_position["Spread_Repub"] = abs(
    pivot_position["Repub_Position_C"] - 
    pivot_position["Repub_Position_L"]
    )

```

Attribution: Used this article: https://www.w3resource.com/pandas/dataframe/dataframe-pivot.php
Asked ChatGPT how to flatten a multi-index column

```{python}

alt.Chart(pivot_position).mark_bar().encode(
    alt.X("US State 2"),
    alt.Y("Spread_Dem"),
    alt.Color("US State 2")
)



```

Merge two datasets together on "US State 2"
```{python}

```

Citation for ANES:  American National Election Studies. 2021. ANES 2020 Time Series 
Study Full Release [dataset and documentation]. July 19, 2021 
version. www.electionstudies.org 


2. Plotting (25%)
    * From that data, you will create a minimum of *two* static plots using `altair` or `geopandas`
    * As well as one `shiny` app with one dynamic plot
        * You can also add additional dynamic plots into your app to substitute for a static plot. So, a `shiny` app with 3 dynamic plots will count for full credit.

```{python}

```

Exploratory graph of polarization
```{python}
alt.data_transformers.enable("vegafusion")
#create list for graph to sort on 
sorting_list = [
        "Refused", "Don't Know", "Extremely Liberal",
        "Liberal", "Slightly Liberal", 
        "Moderate; middle of the road",
        "Slightly conservative",
        "Conservative",
        "Extremely conservative",
        "Haven’t thought much about this"
    ]

alt.Chart(sub_polarization).mark_bar().encode(
    alt.X("Ideology_(V201200):N", sort = sorting_list),
    alt.Y("Observations"),
    alt.Color("Ideology_(V201200)")
)


```

4. Reproductibility (10%)
    * The project and files should be structured and documented so that the TAs can clone your repository and reproduce your results (see "Final Repository" below) by knitting your `.qmd` and, if needed, downloading the dataset(s) you use using the link provided in the `.qmd` comments
5. Git (10%)
    * You should submit your project as a Git repository.
    * Create multiple branches as you work for different pieces of the analysis. Branches may correspond to work done by different partners or to different features if you are working alone.
    * Your final repository should have one branch: `main`
    * We reserve the right to check the git commit history to ensure that all members have contributed to the project.
6. Extra credit: text processing (up to 10%)
    * Introduce some form of text analysis using natural language processing methods discussed in class.

## Writeup (15%)
* You will then spend *no more than 3 pages* writing up your project. 
* The primary purpose of this writeup is to inform us of what we are reading before we look at your code.
* You should describe your research question, then discuss the approach you took and the coding involved, including discussing any weaknesses or difficulties encountered. 
* Display your static plots, and briefly describe them and your Shiny app. Discuss the policy implications of your findings.
* Finish with a discussion of directions for future work. 
* The top of your writeup should include the names of all group members, their respective sections, and Github user names.

## Presentation (15%)
* On the day of the presentation, one of the group members will be *randomly selected* to give a *8-minute in-class presentation*. All group members must be present.
* Any group member who is not present will receive an automatic 0 for the presentation portion of the final project.
* The presentation will be of slides that largely mirror the structure of the writeup, but will be more focused on discussing the research question and results as opposed to explaining the details of the coding. 

# Final Repository
Your final repository must contain the following:

* Documentation and Meta-data
    * A `requirements.txt` file 
    * A `.gitignore` file that ignores unneeded files (e.g. `venv`) 
* Writeup: a user should be able to knit your `.qmd` file and re-generate the HTML version of your writeup
    * The `.qmd` file associated with your write-up
    * An HTML and PDF'd version of your writeup
    * A folder named `pictures` that contains the files for any pictures required to knit your writeup
* Data
    * A folder named `data` that contains the initial, unmodified dataframes you download and the final versions of the dataframe(s) you built.
    * If the dataset is greater than 100MB, it can hosted on Drive or Dropbox and the link should be provided in your .`qmd` file as a comment
* Shiny app
    * A folder named `shiny-app` that contains the code and any additional files needed to deploy your app
    * A user should be able to deploy your app directly from the command line within this folder


# Key Dates
* By November 1
    * Proposal submitted to Canvas quiz
    * (Optional) meeting with Professor Ganong, Professor Shi, or Head TA Ozzy Houck
    * Sign up for presentation slot
* December 2- December 5: in-class presentations
* December 7, 5PM: final repository submitted via Gradescope
